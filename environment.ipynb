{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "colab": {
      "name": "environment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QXlPh08pOL30"
      },
      "source": [
        "# Contextual Bandits for Booking.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_wQVVDT0OL36"
      },
      "source": [
        "\n",
        "### Goal of the project\n",
        "* Implement multiple Agents, such as LinUCB, Eps-Greedy and Deep Regression Model\n",
        "* Apply what we learned in class to our project, but **add something new**\n",
        "* Something new for booking.com could be, e.g. that the margin for different hotels is different. We will consider the margin to be fixed, hence stationary. The reward of the agent is hence not only a rating or whether the user booked or not.\n",
        "* At a later point the margin *could* also change over time turning the problem into a non-stationary problem!\n",
        "* We could also add information about availability, so that the hotel will be booked out after a while.\n",
        "\n",
        "### Setup\n",
        "#### Environment\n",
        "* Gets a list of hotels and then returns the margin that the booking platform made. This depends on:\n",
        "    1. Did the user book the hotel / Did he like it\n",
        "    2. What margin can I make\n",
        "* The margin is hence *did_book* * *margin*\n",
        "#### Agent\n",
        "* Gets a list of hotels and selects one that he will propose to the user\n",
        "\n",
        "#### ToDos\n",
        "* Fix LinUCB\n",
        "* Add constraints to the hotel capacity "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oLxjdSDErwr_"
      },
      "source": [
        "### Tips for working with Tensorflow\n",
        "Use Tensorflow 2. This provides more / better options for debugging\n",
        "```\n",
        "tf.config.experimental_run_functions_eagerly(true)\n",
        "```\n",
        "\n",
        "This allows debugging via\n",
        "```\n",
        "dpb.set_trace()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import copy, deepcopy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_argmax(rng, list_):\n",
        "    \"\"\" similar to np.argmax but return a random element among max\n",
        "        when multiple max exists.\"\"\"\n",
        "    return rng.choice(np.argwhere(list_ == list_.max()).flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyei0iWtOL4f"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExplicitFeedback:\n",
        "    \"\"\" A rating environment with explicit feedback.\n",
        "        User and hotels are represented by points in R^k\n",
        "        User interest in a hotel is modeled by a parametric function\n",
        "        R_{u,h} = f(u,h) = f(W_u, W_h)\n",
        "        Example of function include dot product (cosine similarity), but then take margin into account\n",
        "        f(W_u, W_h) = sigmoid ( \\sum_k w_{u,k} . w_{h,k} ) \n",
        "        R_{u,h} = m_{h} * (f(W_u, W_h) > threshold)\n",
        "        action: Recommend one hotel, which maximizes the margin\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nb_users=30, nb_hotels=10, \n",
        "                 internal_embedding_size=5,\n",
        "                 displayed_users_embedding_size=5,\n",
        "                 displayed_hotels_embedding_size=5,\n",
        "                 noise_size=3,\n",
        "                 threshold=.2,\n",
        "                 max_avail=400,\n",
        "                 seed=None):\n",
        "        self.nb_users = nb_users\n",
        "        self.nb_hotels = nb_hotels\n",
        "        self.internal_embedding_size = internal_embedding_size\n",
        "        self.displayed_users_embedding_size = displayed_users_embedding_size\n",
        "        self.displayed_hotels_embedding_size = displayed_hotels_embedding_size\n",
        "        self.noise_size = noise_size\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        self.threshold = threshold\n",
        "        self.max_avail = max_avail\n",
        "        \n",
        "        self.action_size = self.nb_hotels\n",
        "        self.sampling_limit = self.nb_users * self.nb_hotels\n",
        "        self.user_mean = np.ones(self.internal_embedding_size)\n",
        "        self.user_var = np.ones(self.internal_embedding_size)\n",
        "        self.hotels_mean = np.ones(self.internal_embedding_size)\n",
        "        self.hotels_var = np.ones(self.internal_embedding_size)\n",
        "        self.users_embedding = None\n",
        "        self.hotels_embedding = None\n",
        "        self.user_hotels_history = np.zeros((self.nb_users, self.nb_hotels))\n",
        "        self.z_cut_points = None\n",
        "        self.done = False\n",
        "        ###############################\n",
        "        self.hotel_rooms = np.random.randint(low=self.max_avail/10, high=self.max_avail, size=self.nb_hotels)\n",
        "\n",
        "    def step(self, action):\n",
        "        # check if behind done\n",
        "        if self.done: #self.user_hotels_history.sum() >= self.sampling_limit:\n",
        "            print(\"You are calling step after it return done=True.\\n\"\n",
        "                  \"You should reset the environment.\")\n",
        "\n",
        "        assert action < self.action_size\n",
        "        self.action = action\n",
        "\n",
        "        #update rooms available\n",
        "        self.hotel_rooms[action] = self.hotel_rooms[action] - 1\n",
        "\n",
        "        # see if there are still rooms available\n",
        "        available = np.argwhere(self.hotel_rooms > 0)\n",
        "        if not available.any():\n",
        "          print('All rooms are already booked')\n",
        "          print('Moving to the day after')\n",
        "          self.hotel_rooms = np.random.randint(low=4, high=40, size=self.nb_hotels)\n",
        "        \n",
        "        # compute potential rewards\n",
        "        potential_rewards = [self._get_user_hotels_rating(self.current_user, i) \n",
        "                      for i in np.argwhere(self.hotel_rooms > 0).flatten()]\n",
        "        optimal_return = np.max(potential_rewards)\n",
        "\n",
        "        # map action to hotels\n",
        "        #self.recommended_item = np.argwhere(self.user_item_history[self.current_user, :] == 0)[action][0]       \n",
        "        self.recommended_hotel = np.argwhere(self.hotel_rooms > 0)[action][0]\n",
        "\n",
        "        # mark hotels as rated\n",
        "        self.user_hotels_history[self.current_user, self.recommended_hotel] += 1\n",
        "\n",
        "        # compute reward R_t\n",
        "        self.current_rating = self._get_user_hotels_rating(self.current_user, self.recommended_hotel)\n",
        "        self.reward = self.current_rating\n",
        "        \n",
        "        # check if done\n",
        "        if self.user_hotels_history.sum() == self.sampling_limit:\n",
        "            self.done = True\n",
        "\n",
        "        # compute next state S_{t+1}\n",
        "        self._next_state()\n",
        "\n",
        "        # update action space t+1\n",
        "        self.action_size = len(self.available_hotels)\n",
        "\n",
        "        return self.reward, self.state, self.done, optimal_return\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        self.action_size = self.nb_hotels\n",
        "        self.hotel_rooms = np.random.randint(low=self.max_avail/10, high=self.max_avail, size=self.nb_hotels)\n",
        "\n",
        "        \n",
        "        # create users and hotels embedding matrix\n",
        "        self.users_embedding = self._rng.normal(loc=self.user_mean,\n",
        "                                                scale=self.user_var,\n",
        "                                                size=(self.nb_users, self.internal_embedding_size))\n",
        "        self.hotels_embedding = self._rng.normal(loc=self.hotels_mean,\n",
        "                                                scale=self.hotels_var,\n",
        "                                                size=(self.nb_hotels, self.internal_embedding_size))\n",
        "        self.margin = self._rng.rand(self.nb_hotels)\n",
        "\n",
        "\n",
        "        # Let X = users_embedding and Y = hotels_embedding\n",
        "        # In order to properly map float into integers, we need to know the distribution of\n",
        "        # Z = \\sum_k X_k.Y_k\n",
        "        # E[Z] = \\sum_k E[X_k.Y_k] = \\sum_k E[X_k]E[Y_k]\n",
        "        # Var[Z] = \\sum_k Var[X_k.Y_k] = \\sum_k Var[X_k]Var[Y_k] + Var[X_k]E[Y_k]^2 + Var[Y_k]E[X_k]^2\n",
        "        z_mean = self.user_mean.dot(self.hotels_mean)\n",
        "        z_var = self.user_var.dot(self.hotels_var) + self.user_var.dot(np.square(self.hotels_mean)) + \\\n",
        "                self.hotels_var.dot(np.square(self.user_mean))\n",
        "        z = norm(z_mean, np.sqrt(z_var))\n",
        "        # to get 5 values, we need 4 cut points\n",
        "        self.z_cut_points = z.ppf([self.threshold]) # 0.2, 0.4, 0.6, 0.8 # you can control the distribution of ratings here.\n",
        "        self.user_hotels_history = np.zeros((self.nb_users, self.nb_hotels))\n",
        "        self.done = False\n",
        "\n",
        "        self._next_state()\n",
        "        return self.state\n",
        "\n",
        "    def _get_user_hotels_rating(self, user, hotel):\n",
        "        real_score = (self.users_embedding[user].dot(self.hotels_embedding[hotel]))   \n",
        "        booking_score = np.searchsorted(self.z_cut_points, real_score)\n",
        "        return int((booking_score * self.margin[hotel] * 100) / 10)\n",
        "\n",
        "    def _get_variables(self, user, hotel):\n",
        "        user_embedding = self.users_embedding[user]\n",
        "        hotel_embedding = self.hotels_embedding[hotel]\n",
        "        if self.displayed_users_embedding_size + self.displayed_hotels_embedding_size > 0:\n",
        "            variables = np.array([user_embedding[:self.displayed_users_embedding_size],\n",
        "                                  hotel_embedding[:self.displayed_hotels_embedding_size]])\n",
        "\n",
        "            if self.noise_size > 0:\n",
        "                noise = self._rng.normal(loc=np.ones(self.noise_size),\n",
        "                                         scale=np.ones(self.noise_size),\n",
        "                                         size=self.noise_size)\n",
        "                variables = np.append(variables, noise)\n",
        "\n",
        "            return variables\n",
        "\n",
        "    def _get_new_user(self):\n",
        "        for i in range(10):\n",
        "            user = self._rng.randint(0, self.nb_users)\n",
        "            # check it remain at least one hotel\n",
        "            if np.sum(self.user_hotels_history[user, :]) < self.nb_hotels:\n",
        "                return user\n",
        "        return self._rng.choice(np.argwhere(self.user_hotels_history <= self.max_vail))[0]\n",
        "\n",
        "    def _next_state(self):\n",
        "        # Pick a user\n",
        "        if self.user_hotels_history.sum() < self.sampling_limit:\n",
        "            self.current_user = self._get_new_user()\n",
        "        else:\n",
        "            self.current_user = None\n",
        "\n",
        "        # List available hotels\n",
        "        self.available_hotels = np.argwhere(self.user_hotels_history[self.current_user, :] <= self.max_avail)\n",
        "        #print(self.available_hotels)\n",
        "        #\n",
        "\n",
        "        self.state = list()\n",
        "        for i in self.available_hotels:\n",
        "            hotel = i[0]\n",
        "            # Compute variables\n",
        "            variables = self._get_variables(self.current_user, hotel)\n",
        "            self.state.append([self.current_user, hotel, variables])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ExplicitFeedback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "[[0, 0, array([-0.77,  1.08, -0.13,  0.35,  0.11,  1.56,  0.81, -0.43,  1.46,\n          1.93,  2.38,  0.61,  1.08])],\n [0, 1, array([-0.77,  1.08, -0.13,  0.35,  0.11,  2.02,  1.49,  0.2 ,  0.56,\n          3.11,  1.66,  0.27,  1.16])],\n [0, 2, array([-0.77,  1.08, -0.13,  0.35,  0.11,  0.28,  2.7 ,  0.31,  0.14,\n          3.2 ,  1.2 ,  2.09, -0.03])],\n [0, 3, array([-0.77,  1.08, -0.13,  0.35,  0.11,  0.28,  1.25,  1.3 ,  1.18,\n          2.07, -0.62,  1.64,  3.12])],\n [0, 4, array([-0.77,  1.08, -0.13,  0.35,  0.11, -0.86,  1.45,  0.84, -0.53,\n          0.75,  0.42, -0.1 ,  2.06])],\n [0, 5, array([-0.77,  1.08, -0.13,  0.35,  0.11,  1.23,  3.47,  1.63,  0.58,\n          2.01,  0.72,  1.51,  1.44])],\n [0, 6, array([-0.77,  1.08, -0.13,  0.35,  0.11, -0.83,  0.64, -0.09,  1.04,\n          1.32,  2.76,  1.63,  0.98])],\n [0, 7, array([-0.77,  1.08, -0.13,  0.35,  0.11, -0.33,  0.81,  0.56,  1.53,\n          1.3 , -1.26,  1.12,  1.79])],\n [0, 8, array([-0.77,  1.08, -0.13,  0.35,  0.11,  1.56,  2.02,  3.1 ,  2.04,\n          0.53, -0.04, -0.1 ,  2.6 ])],\n [0, 9, array([-0.77,  1.08, -0.13,  0.35,  0.11,  1.27, -0.04, -1.06, -0.14,\n          1.5 ,  1.62,  1.93,  1.12])]]"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.reset(seed=2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PAdBaPrROL45"
      },
      "source": [
        "user 11 is connecting to your platform and we shoud recommend him one item among the ten availables.\n",
        "We also observe a vector of features that could depend on the user, the item and/or some context(like time, weather, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "reward:  0\n"
        }
      ],
      "source": [
        "reward, next_state, done, optimal_return = env.step(3)\n",
        "print('reward: ', reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rltxm8gGOL5H"
      },
      "source": [
        "We recommend the first hotel of the list (hotel 0) to the user and the margin as reward. If the margin is 0 he did not book it!\n",
        "\n",
        "We also get the next state, that is the next user connect to our application, the list of available hotels for recommendations and a list of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "[[25, 0, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  1.56,  0.81, -0.43,  1.46,\n          1.93,  2.54, -0.59,  0.74])],\n [25, 1, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  2.02,  1.49,  0.2 ,  0.56,\n          3.11, -0.  ,  0.9 , -0.38])],\n [25, 2, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  0.28,  2.7 ,  0.31,  0.14,\n          3.2 ,  1.45,  1.84,  1.81])],\n [25, 3, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  0.28,  1.25,  1.3 ,  1.18,\n          2.07,  1.3 ,  2.02,  0.4 ])],\n [25, 4, array([ 0.66,  2.24,  1.22, -0.11,  0.39, -0.86,  1.45,  0.84, -0.53,\n          0.75,  2.77, -0.63, -0.52])],\n [25, 5, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  1.23,  3.47,  1.63,  0.58,\n          2.01,  0.6 ,  1.49,  1.01])],\n [25, 6, array([ 0.66,  2.24,  1.22, -0.11,  0.39, -0.83,  0.64, -0.09,  1.04,\n          1.32,  0.44,  2.52,  0.06])],\n [25, 7, array([ 0.66,  2.24,  1.22, -0.11,  0.39, -0.33,  0.81,  0.56,  1.53,\n          1.3 ,  0.77,  0.72,  2.2 ])],\n [25, 8, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  1.56,  2.02,  3.1 ,  2.04,\n          0.53, -0.21,  0.98,  0.47])],\n [25, 9, array([ 0.66,  2.24,  1.22, -0.11,  0.39,  1.27, -0.04, -1.06, -0.14,\n          1.5 ,  2.46,  0.41,  1.73])]]"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E83Rz9aGOL5Q"
      },
      "source": [
        "### User-Hotel recommender system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "swFMM9svOL5S"
      },
      "source": [
        "Let start with a recommender system that use only user_id and hotel_id.\n",
        "\n",
        "But before let generate some historical data from a random agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I_HWT_CbOL5s"
      },
      "source": [
        "## Run experiment\n",
        "In order to make Agent and Environment interract, we can create an experiment, parametrized by the number of step we will be running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_exp(agent, env, nb_steps, env_seed):\n",
        "    rewards = np.zeros(nb_steps)\n",
        "    regrets = np.zeros(nb_steps)\n",
        "    actions = np.zeros(nb_steps)  #hotels\n",
        "    users   = np.zeros(nb_steps) \n",
        "    context = env.reset(env_seed)\n",
        "    rating_matrix = np.zeros((env.nb_users, env.nb_hotels))\n",
        "    for i in range(nb_steps):\n",
        "        # Select action from agent policy.\n",
        "        action = agent.act(context)\n",
        "        \n",
        "        # Play action in the environment and get reward.\n",
        "        reward, next_context, done, optimal_return = env.step(action)\n",
        "        \n",
        "        # Update history\n",
        "        user = context[0][0]\n",
        "        item = context[action][1]\n",
        "        rating = reward\n",
        "        rating_matrix[user, item] = rating\n",
        "        \n",
        "        # Update agent.\n",
        "        agent.update(context, action, reward)\n",
        "        context = next_context\n",
        "        \n",
        "        # Save history.\n",
        "        #context[i] = context\n",
        "        rewards[i] = reward\n",
        "        actions[i] = action\n",
        "        users[i]   = user\n",
        "        regrets[i] = optimal_return - reward\n",
        "\n",
        "    reward = rewards.sum()\n",
        "    regret = np.sum(regrets)\n",
        "    return {'reward': reward, \n",
        "            'regret': regret,\n",
        "            'rewards': rewards,\n",
        "            'users' : users,            \n",
        "            'regrets': regrets,\n",
        "            'actions': actions,\n",
        "            'cum_rewards': np.cumsum(rewards), \n",
        "            'cum_regrets': np.cumsum(regrets),\n",
        "            'rating_matrix': rating_matrix\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FNvAAXwcOL6K"
      },
      "source": [
        "You can see that on this experiment the total reward is 285 (401) and the regret is 112 (495). You can also have a look at individual rewards, actions or regrets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AikAyyVZOL6M"
      },
      "source": [
        "Thanks to the historical data, we can train a matrix factorization algorithm to try to predict the non observed rating values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azKFXOmoOL5X"
      },
      "source": [
        "## Agent\n",
        "Here we create a very basic agent that will pull arm, i.e.play action, at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Random:\n",
        "    \"\"\" Random agent. \"\"\"\n",
        "    def __init__(self, nb_arms, seed=None):\n",
        "        self._nb_arms = nb_arms\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        \n",
        "    def act(self, context):\n",
        "        action = self._rng.randint(len(context)) # note that action size is changing\n",
        "        return action\n",
        "        \n",
        "    def update(self, context, action, reward):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = Random(None, seed=2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_exp = 100\n",
        "nb_steps = 100\n",
        "regret = np.zeros(nb_exp)\n",
        "regrets = np.zeros((nb_exp, nb_steps))\n",
        "for i in range(nb_exp):\n",
        "    env = ExplicitFeedback()\n",
        "    agent = Random(None, seed=i)\n",
        "    exp = run_exp(agent, env, nb_steps, env_seed=i)\n",
        "    regret[i] = exp['regret'] \n",
        "    regrets[i] = exp['cum_regrets']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(regrets.mean(axis=0), color='blue')\n",
        "plt.plot(np.quantile(regrets, 0.05,axis=0), color='grey', alpha=0.5)\n",
        "plt.plot(np.quantile(regrets, 0.95,axis=0), color='grey', alpha=0.5)\n",
        "plt.title('Mean regret: {:.2f}'.format(regret.mean()))\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('regret')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CFY0RxlGOL6U"
      },
      "source": [
        "## Epsilon Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EpsilonGreedy:\n",
        "    \"\"\" Epsilon greedy agent. \"\"\"\n",
        "    def __init__(self, nb_arms, context_size, lr=.1, epsilon=0, seed=None):\n",
        "        self._nb_arms = nb_arms\n",
        "        self._p = context_size\n",
        "        self._lr = lr\n",
        "        self._epsilon = epsilon\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        self._beta = np.zeros((nb_arms, self._p)) \n",
        "        self._n = np.zeros(nb_arms)\n",
        "        self._pred_reward = np.zeros(nb_arms)\n",
        "        \n",
        "    def act(self, context):\n",
        "        ###############\n",
        "        context_v = np.asarray(context)[:,2]\n",
        "        context_f = []\n",
        "        for i in context_v:\n",
        "            context_f.append(np.asarray(i))\n",
        "        context_f = np.asarray(context_f)\n",
        "        ###############\n",
        "        #print('c', context_f.shape)\n",
        "        #print('b', self._beta.shape)\n",
        "\n",
        "        if self._rng.rand() < self._epsilon:\n",
        "            action = self._rng.randint(self._nb_arms)\n",
        "        else:\n",
        "            pred_reward = np.einsum('ij,ij->i', context_f, self._beta)\n",
        "            action = random_argmax(self._rng, pred_reward)\n",
        "        return action\n",
        "        \n",
        "    def update(self, context, action, reward):\n",
        "        \"\"\" Simple gradient descent. \"\"\"\n",
        "        ################\n",
        "        context_v = np.asarray(context)[:,2]\n",
        "        context_f = []\n",
        "        for i in context_v:\n",
        "            context_f.append(np.asarray(i))\n",
        "        context_f = np.asarray(context_f)\n",
        "        ################\n",
        "        \n",
        "        self._n[action] += 1\n",
        "        grad = - context_f[action] * (reward - context_f[action].dot(self._beta[action]))\n",
        "        self._beta[action] = self._beta[action] - self._lr/self._n[action] * grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eps_agent = EpsilonGreedy(10, 13)\n",
        "run_exp(eps_agent, env, nb_steps=100, env_seed=2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a0WE98PmOL6R"
      },
      "source": [
        "<font color='red'> For the rest of this notebook, we will assume that when the Agent start it has access to some historical data generated by a random policy. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_exp = 500\n",
        "nb_steps = 100\n",
        "regret = np.zeros(nb_exp)\n",
        "regrets = np.zeros((nb_exp, nb_steps))\n",
        "for i in range(nb_exp):\n",
        "    env.reset(seed=2020)\n",
        "    eps_agent = EpsilonGreedy(10, 13)\n",
        "    res = run_exp(eps_agent, env, nb_steps, env_seed=2020)\n",
        "    regret[i] = res['regret'] \n",
        "    regrets[i] = res['cum_regrets']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(regrets.mean(axis=0), color='blue')\n",
        "plt.plot(np.quantile(regrets, 0.05, axis=0), color='grey', alpha=0.5)\n",
        "plt.plot(np.quantile(regrets, 0.95, axis=0), color='grey', alpha=0.5)\n",
        "plt.title('Mean regret: {:.2f}'.format(regret.mean()))\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('regret')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UZayNx4zjIpR"
      },
      "source": [
        "## LinUCB (to completE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinUCB():\n",
        "\n",
        "  def __init__(self, T, d, delta=0.1):\n",
        "    self.T = T\n",
        "    self.sigma = 1                    # subgaussianity\n",
        "    self.d = d                        # dimension\n",
        "\n",
        "    self.lambd =  0.1               # regularization  < 1\n",
        "    self.L = 1                       # upperbound for actions\n",
        "    self.S = 1                        # upperbound for parameters 0.1\n",
        "    self.alpha = 0.8                    # weight [default = 1] 1\n",
        "    self.t = 0                         # iteration number\n",
        "    self.delta = delta\n",
        "     \n",
        "\n",
        "\n",
        "  def start(self):\n",
        "\n",
        "    self.t = 0\n",
        "    self.V = np.identity(self.d) * self.lambd          # covariance or design matrix\n",
        "    self.b = np.zeros(self.d)\n",
        "    self.theta = np.zeros(self.d)\n",
        "    self.V_inv = np.linalg.inv(self.V)          \n",
        "    \n",
        "\n",
        "  def act(self, context): #(self, arms)\n",
        "    ###############\n",
        "    context_v = np.asarray(context)[:,2]\n",
        "    context_f = []\n",
        "    for i in context_v:\n",
        "        context_f.append(np.asarray(i))\n",
        "    arms = np.asarray(context_f)\n",
        "    ###############\n",
        "\n",
        "    ucb = []\n",
        "\n",
        "    part1 = np.sqrt(self.lambd) * self.S\n",
        "    part2 = np.sqrt(2*np.log(1/self.delta) + self.d * np.log( (self.lambd * self.d + self.t * self.L**2)/ (self.lambd * self.d) ))\n",
        "    #part3 = np.sqrt(2*np.log(1/delta) + self.d * np.log( np.linalg.det(self.V)/ self.lambd**self.d) )\n",
        "    self.beta = part1 + part2\n",
        "    self.t = self.t + 1\n",
        "    \n",
        "    for act in arms:\n",
        "      print(act.shape, self.V_inv.shape)\n",
        "      elliptic_norm = np.matmul(np.matmul(act.T,  self.V_inv), act)\n",
        "      ucb_value = np.matmul(act.T, self.theta) + self.alpha * np.sqrt( self.beta ) * np.sqrt( elliptic_norm )\n",
        "      ucb.append(ucb_value)\n",
        "    \n",
        "    ucb = np.asarray(ucb)\n",
        "    #ucb[np.random.choice(len(ucb))] += 0.000001  # per valori uguali\n",
        "    action = np.argmax(ucb)\n",
        "    print('act', action)\n",
        "    return action\n",
        "\n",
        "\n",
        "  def update(self, context, action, reward):\n",
        "    chosen_arm = context\n",
        "    self.V = self.V + np.matmul(chosen_arm, chosen_arm.T)\n",
        "    self.V_inv = self.V_inv - ((self.V_inv * chosen_arm * chosen_arm.T * self.V_inv) / (1 + chosen_arm.T * self.V_inv * chosen_arm))\n",
        "    self.b = self.b + reward * chosen_arm\n",
        "    self.theta = np.matmul(self.V_inv, self.b)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linucb = LinUCB(10, 13)\n",
        "linucb.start()\n",
        "run_exp(linucb, env, nb_steps=100, env_seed=2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WpQs77qXnNVW"
      },
      "source": [
        "## Thompson agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BetaBernoulli(object):\n",
        "    \"\"\" Beta Bernoulli Bayesian distribution. \"\"\"\n",
        "    def __init__(self, a=1, b=1, prior=np.ones(10)):\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.n = copy(prior)  # number of {0,1} rewards\n",
        "\n",
        "    def update(self, reward):\n",
        "        self.n[int(reward)] += 1\n",
        "\n",
        "    def sample(self, np_random):\n",
        "        return np_random.beta(self.a + self.n[1], self.b + self.n[0])\n",
        "\n",
        "from scipy.stats import t as student\n",
        "class NormalGamma(object):\n",
        "    \"\"\" NG(m, λ|µ = ·, κ = 0, α = −1/2, β = 0) \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mean = 0\n",
        "        self.ss = 0\n",
        "        self.n = 0\n",
        "\n",
        "    def update(self, reward):\n",
        "        self.n += 1\n",
        "        # update mean and sum of square\n",
        "        old_mean = self.mean\n",
        "        self.mean +=  (reward - self.mean)/self.n\n",
        "        self.ss += (reward - old_mean) * (reward - self.mean) # Welford's algorithm\n",
        "\n",
        "    def sample(self, np_random):\n",
        "        if self.n <= 2:\n",
        "            return np.Inf\n",
        "        else:\n",
        "            return student.rvs(df=self.n-1, loc=self.mean, \n",
        "                               scale=self.ss/(self.n*self.n-1),\n",
        "                               random_state=np_random)\n",
        "\n",
        "class Thompson_agent():\n",
        "    def __init__(self, nb_arms, a=1, b=1, seed=None):\n",
        "        self._nb_arms = nb_arms\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        #self.dist = BetaBernoulli(a,b, prior =np.ones(nb_arms))\n",
        "        dist=NormalGamma\n",
        "        self._posterior = [dist() for i in range(self._nb_arms)]\n",
        "        \n",
        "    def act(self, context):\n",
        "        sample = np.array([self._posterior[i].sample(self._rng)\n",
        "                           for i in range(self._nb_arms)])\n",
        "        action = np.argmax(sample)\n",
        "        return action\n",
        "        \n",
        "    def update(self, context, action, reward):\n",
        "        self._posterior[action].update(reward)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thompson = Thompson_agent(10)\n",
        "run_exp(thompson, env, nb_steps=100, env_seed=2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_exp = 1000\n",
        "nb_steps = 100\n",
        "regret = np.zeros(nb_exp)\n",
        "regrets = np.zeros((nb_exp, nb_steps))\n",
        "for i in range(nb_exp):\n",
        "    env.reset(seed=2020)\n",
        "    thompson = Thompson_agent(10)\n",
        "    res = run_exp(thompson, env, nb_steps, env_seed=2020)\n",
        "    regret[i] = res['regret'] \n",
        "    regrets[i] = res['cum_regrets']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(regrets.mean(axis=0), color='blue')\n",
        "plt.plot(np.quantile(regrets, 0.05, axis=0), color='grey', alpha=0.5)\n",
        "plt.plot(np.quantile(regrets, 0.95, axis=0), color='grey', alpha=0.5)\n",
        "plt.title('Mean regret: {:.2f}'.format(regret.mean()))\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('regret')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BcXbfgj9OL6z"
      },
      "source": [
        "## Recommender System with historical data: Embedding Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Dot, Concatenate\n",
        "import tensorflow as tf\n",
        "\n",
        "class RegressionModel(Model):\n",
        "    def __init__(self, embedding_size, nb_users, nb_hotels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
        "                                        input_dim=nb_users,\n",
        "                                        input_length=1,\n",
        "                                        name='user_embedding')\n",
        "        self.hotel_embedding = Embedding(output_dim=embedding_size,\n",
        "                                        input_dim=nb_hotels,\n",
        "                                        input_length=1,\n",
        "                                        name='hotel_embedding')\n",
        "        self.flatten = Flatten()\n",
        "        self.dot = Dot(axes=1)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        user_inputs = inputs[0]\n",
        "        hotel_inputs = inputs[1]\n",
        "        \n",
        "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
        "        hotel_vecs = self.flatten(self.hotel_embedding(hotel_inputs))\n",
        "        \n",
        "        y = self.dot([user_vecs, hotel_vecs])\n",
        "\n",
        "        # Multiplying it by 10 making it closer to our rewards helps to speed up training\n",
        "        a = tf.constant([10.])\n",
        "        y = tf.multiply(a,y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingAgent:\n",
        "    def __init__(self, X, Y, nb_users, nb_hotels):\n",
        "        self._model = RegressionModel(64, nb_users, nb_hotels)\n",
        "        self._model.compile(optimizer=\"adam\", loss='mae')\n",
        "        self._model.fit(X, Y,batch_size=64, epochs=120, validation_split=0.1,shuffle=True)\n",
        "        self._user_embeddings = self._model.get_weights()[0]\n",
        "        self._hotels_embeddings = self._model.get_weights()[1]\n",
        "    \n",
        "    def act(self, context):\n",
        "        user = context[0][0]\n",
        "        user_embedding = self._user_embeddings[user]\n",
        "        dot_products = np.dot(self._hotels_embeddings, user_embedding)\n",
        "        best_hotels = np.argsort(dot_products)[::-1] #[::-1] reverses order so now the element with the highest expected reward is at first position\n",
        "        hotel_selected = best_hotels[0]\n",
        "        return hotel_selected\n",
        "\n",
        "    def update(self, context, action, reward):\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## General way to train the Embedding Agent by first creating Historical data and then training with it\n",
        "steps = 250\n",
        "\n",
        "eps_agent = EpsilonGreedy(10, 13)\n",
        "rand_agent = Random(10)\n",
        "\n",
        "#to create historical data\n",
        "res = run_exp(rand_agent, env, nb_steps=steps, env_seed=2020)\n",
        "users_values = res['users']\n",
        "hotels_values = res['actions']\n",
        "rewards_values = res['rewards']\n",
        "############\n",
        "\n",
        "Y = rewards_values\n",
        "X = [users_values, hotels_values]\n",
        "\n",
        "embagent = EmbeddingAgent(X, Y, env.nb_users, env.nb_hotels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_exp = 10\n",
        "nb_steps = 100\n",
        "regret = np.zeros(nb_exp)\n",
        "regrets = np.zeros((nb_exp, nb_steps))\n",
        "for i in range(nb_exp):\n",
        "   \n",
        "    # Generate historical data and train model\n",
        "    steps = 30\n",
        "\n",
        "    eps_agent = EpsilonGreedy(10, 13)\n",
        "    rand_agent = Random(10)\n",
        "\n",
        "    seed = np.random.randint(1000)\n",
        "\n",
        "    res = run_exp(rand_agent, env, nb_steps=steps, env_seed=seed)\n",
        "    users_values = res['users']\n",
        "    hotels_values = res['actions']\n",
        "    rewards_values = res['rewards']\n",
        "    \n",
        "\n",
        "    Y = rewards_values\n",
        "    X = [users_values, hotels_values]\n",
        "\n",
        "    embagent = EmbeddingAgent(X, Y, env.nb_users, env.nb_hotels)\n",
        "    ############\n",
        "\n",
        "    # Run experiment\n",
        "    res = run_exp(embagent, env, nb_steps, env_seed=seed)\n",
        "    regret[i] = res['regret'] \n",
        "    regrets[i] = res['cum_regrets']\n",
        "\n",
        "\n",
        "plt.plot(regrets.mean(axis=0), color='blue')\n",
        "plt.plot(np.quantile(regrets, 0.05,axis=0), color='grey', alpha=0.5)\n",
        "plt.plot(np.quantile(regrets, 0.95,axis=0), color='grey', alpha=0.5)\n",
        "plt.title('Mean regret: {:.2f}'.format(regret.mean()))\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('regret')\n",
        "plt.show()"
      ]
    }
  ]
}